{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Covid_Diagnosis_Using_CT_Scans.ipynb","provenance":[{"file_id":"1vLhahBZRL9zz9H6mm2QZViqBr6yOj66y","timestamp":1619721873718}],"collapsed_sections":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","metadata":{"id":"RsptssR3Aonf"},"source":["### **Classification of CT scan images as either covid or healthy**"]},{"cell_type":"markdown","metadata":{"id":"_L1yjaqQPlrF"},"source":["Installations and imports"]},{"cell_type":"code","metadata":{"id":"eOnQom4a2v06","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1642361462640,"user_tz":360,"elapsed":8221,"user":{"displayName":"Matthew Boraske","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gjuu-JGZEUwBYJ4mDYIPdXiP3VutrpOg_K7sJDIvQ=s64","userId":"02919232727119503184"}},"outputId":"f3f0faa6-21e5-4650-ffb7-a727587cf71c"},"source":["#installation of pillow for image resizing \n","!pip install --upgrade pip\n","!pip install --upgrade Pillow"],"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: pip in /usr/local/lib/python3.7/dist-packages (21.3.1)\n","\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n","Requirement already satisfied: Pillow in /usr/local/lib/python3.7/dist-packages (9.0.0)\n","\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n"]}]},{"cell_type":"code","metadata":{"id":"AJUhEOkd_L7t","executionInfo":{"status":"ok","timestamp":1642361467631,"user_tz":360,"elapsed":1222,"user":{"displayName":"Matthew Boraske","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gjuu-JGZEUwBYJ4mDYIPdXiP3VutrpOg_K7sJDIvQ=s64","userId":"02919232727119503184"}}},"source":["##USEFUL LIBRARIES\n","import numpy as np\n","import scipy\n","import numpy as np\n","import matplotlib.pyplot as plt\n","from matplotlib import image\n","from sklearn.model_selection import train_test_split\n","from sklearn.datasets import load_digits\n","from sklearn.ensemble import RandomForestClassifier\n","from sklearn.feature_selection import RFE\n","import glob\n","import os\n","from PIL import Image\n","import zipfile"],"execution_count":2,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"K77DwhvYPrNG"},"source":["Mounting of google drive to get data"]},{"cell_type":"code","metadata":{"id":"SJI1FhF5qydU","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1642361581940,"user_tz":360,"elapsed":24486,"user":{"displayName":"Matthew Boraske","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gjuu-JGZEUwBYJ4mDYIPdXiP3VutrpOg_K7sJDIvQ=s64","userId":"02919232727119503184"}},"outputId":"e507c8ed-bfa1-478a-a0b2-b54810c8e497"},"source":["##MOUNTING GOOGLE DRIVE TO THIS COLAB NOTEBOOK\n","from google.colab import drive\n","drive._mount('/content/drive')  "],"execution_count":4,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}]},{"cell_type":"markdown","metadata":{"id":"XfArXc2MSLu0"},"source":["Importing images and converting them into 1-D RGB image matrices"]},{"cell_type":"code","metadata":{"id":"ChO8STT2sTRB","executionInfo":{"status":"ok","timestamp":1642361720787,"user_tz":360,"elapsed":9948,"user":{"displayName":"Matthew Boraske","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gjuu-JGZEUwBYJ4mDYIPdXiP3VutrpOg_K7sJDIvQ=s64","userId":"02919232727119503184"}}},"source":["##UNZIPPING THE DATA\n","zip_ref = zipfile.ZipFile(\"/content/drive/MyDrive/HCML project/archive.zip\",'r')\n","zip_ref.extractall(\"/content/dataset\")\n","zip_ref.close()"],"execution_count":6,"outputs":[]},{"cell_type":"code","metadata":{"id":"9JSPZ28d774m","executionInfo":{"status":"ok","timestamp":1642361724515,"user_tz":360,"elapsed":277,"user":{"displayName":"Matthew Boraske","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gjuu-JGZEUwBYJ4mDYIPdXiP3VutrpOg_K7sJDIvQ=s64","userId":"02919232727119503184"}}},"source":["##CONVERTS A PNG INTO AN RGB MATRIX\n","def image_to_matrix(image_file, grays=False):\n","    img = image.imread(image_file)\n","    if(len(img.shape) == 3 and img.shape[2] > 3):\n","        height, width, depth = img.shape\n","        new_img = np.zeros([height, width, 3])\n","        for r in range(height):\n","            for c in range(width):\n","                new_img[r,c,:] = img[r,c,0:3]\n","        img = np.copy(new_img)\n","    if(grays and len(img.shape) == 3):\n","        height, width = img.shape[0:2]\n","        new_img = np.zeros([height, width])\n","        for r in range(height):\n","            for c in range(width):\n","                new_img[r,c] = img[r,c,0]\n","        img = new_img\n","    if(len(img.shape) == 2):\n","        zeros = np.where(img == 0)[0]\n","        img[zeros] += 1e-7\n","    return img"],"execution_count":7,"outputs":[]},{"cell_type":"code","metadata":{"id":"WLvLWiVSYZKJ","executionInfo":{"status":"ok","timestamp":1642361727002,"user_tz":360,"elapsed":409,"user":{"displayName":"Matthew Boraske","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gjuu-JGZEUwBYJ4mDYIPdXiP3VutrpOg_K7sJDIvQ=s64","userId":"02919232727119503184"}}},"source":["def flatten(image_matrix):\n","    if(len(image_matrix.shape) == 3):\n","        height, width, depth = image_matrix.shape\n","    else:\n","        height, width = image_matrix.shape\n","        depth = 1\n","    flattened_values = np.zeros([height*width,depth])\n","    for i, r in enumerate(image_matrix):\n","        for j, c in enumerate(r):\n","            flattened_values[i*width+j,:] = c\n","    oneDim = []\n","    for pixel in range(len(flattened_values)):\n","      for RGB_value in range(3):\n","        if RGB_value == 0:\n","          oneDim.append(flattened_values[pixel][RGB_value])\n","    return np.array(oneDim)"],"execution_count":8,"outputs":[]},{"cell_type":"code","metadata":{"id":"COxpX7Ws2Vgw","executionInfo":{"status":"ok","timestamp":1642361728689,"user_tz":360,"elapsed":306,"user":{"displayName":"Matthew Boraske","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gjuu-JGZEUwBYJ4mDYIPdXiP3VutrpOg_K7sJDIvQ=s64","userId":"02919232727119503184"}}},"source":["#Resize image function\n","def resize_image(image_file, width, height):\n","  image = Image.open(image_file)\n","  new_image = image.resize((width, height))\n","  new_image.save(image_file)\n","  return image_file"],"execution_count":9,"outputs":[]},{"cell_type":"code","metadata":{"id":"BW0W4AYbDga_","colab":{"base_uri":"https://localhost:8080/"},"outputId":"b11fbc2b-cc4e-4e3d-bee1-96cbb6958019"},"source":["#reading in CT images and converting them to RGB matrices\n","#########################################################\n","from PIL import Image\n","\n","#getting patient folders (both covid and healthy)\n","patient_folders = []\n","target=[]\n","for filepath in glob.glob(os.path.join('/content/dataset/New_Data_CoV2/Covid', '*')):\n","  patient_folders.append(filepath)\n","  target.append(\"Covid\")\n","\n","healthy_patient_folders = []\n","for filepath in glob.glob(os.path.join('/content/dataset/New_Data_CoV2/Healthy', '*')):\n","  patient_folders.append(filepath)\n","  target.append(\"Healthy\")\n","\n","#convert folder lists to arrays\n","patient_folders = np.array(patient_folders)\n","\n","#split the folders in training/testing sets\n","folder_train, folder_test, y_train, y_test = train_test_split(patient_folders, target, test_size = 0.2, random_state = 0) \n","\n","#Convert training images to matrices and add them to a list of them\n","print(\"Beginning conversion of training images to matrices\")\n","train_image_matrix = []\n","train_target = []\n","for i in range(len(folder_train)): ##len(folder_train)\n","  folder_filepath = folder_train[i]\n","  category = y_train[i]\n","  for png_filepath in glob.glob(os.path.join(folder_filepath, '*.png')):\n","      img_filepath = resize_image(png_filepath,280,200)\n","      train_image_matrix.append(flatten(image_to_matrix(img_filepath)))\n","      train_target.append(category)\n","  print(\"\\tAdding images folder:\", folder_filepath)\n","\n","#Convert testing images to matrices and add them to a list of them\n","print(\"Beginning conversion of testing images to matrices\")\n","test_image_matrix = []\n","test_target = []\n","for i in range(len(folder_test)):\n","  folder_filepath = folder_test[i]\n","  category = y_test[i]\n","  for png_filepath in glob.glob(os.path.join(folder_filepath, '*.png')):\n","      img_filepath = resize_image(png_filepath,280,200)\n","      test_image_matrix.append(flatten(image_to_matrix(img_filepath)))\n","      test_target.append(category)\n","  print(\"\\tAdding images folder:\", folder_filepath)\n","_ \n","#convert image_matrix lists to arrays\n","train_image_matrix = np.array(train_image_matrix)\n","test_image_matrix = np.array(test_image_matrix)\n","train_target = np.array(train_target)\n","test_target = np.array(test_target)\n","print(\"Conversion of images to matrices is complete\")\n","\n","#print that conversion process is complete and size of the training and testing image matrices\n","print(\"Training images matrix dimensions:\", train_image_matrix.shape)\n","print(\"Testing images matrix dimensions:\", test_image_matrix.shape)\n","\n","print(\"Training images target matrix dimensions:\", train_target.shape)\n","print(\"Testing image target matrix dimensions:\", test_target.shape)"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Beginning conversion of training images to matrices\n","\tAdding images folder: /content/dataset/New_Data_CoV2/Healthy/Patient (2)\n","\tAdding images folder: /content/dataset/New_Data_CoV2/Covid/Patient (15)\n","\tAdding images folder: /content/dataset/New_Data_CoV2/Covid/Patient (47)\n","\tAdding images folder: /content/dataset/New_Data_CoV2/Covid/Patient (79)\n","\tAdding images folder: /content/dataset/New_Data_CoV2/Covid/Patient (2)\n","\tAdding images folder: /content/dataset/New_Data_CoV2/Healthy/Patient (37)\n","\tAdding images folder: /content/dataset/New_Data_CoV2/Covid/Patient (59)\n","\tAdding images folder: /content/dataset/New_Data_CoV2/Covid/Patient (14)\n","\tAdding images folder: /content/dataset/New_Data_CoV2/Covid/Patient (63)\n","\tAdding images folder: /content/dataset/New_Data_CoV2/Covid/Patient (17)\n","\tAdding images folder: /content/dataset/New_Data_CoV2/Covid/Patient (25)\n","\tAdding images folder: /content/dataset/New_Data_CoV2/Healthy/Patient (16)\n","\tAdding images folder: /content/dataset/New_Data_CoV2/Healthy/Patient (30)\n","\tAdding images folder: /content/dataset/New_Data_CoV2/Healthy/Patient (23)\n","\tAdding images folder: /content/dataset/New_Data_CoV2/Covid/Patient (65)\n","\tAdding images folder: /content/dataset/New_Data_CoV2/Covid/Patient (20)\n","\tAdding images folder: /content/dataset/New_Data_CoV2/Covid/Patient (68)\n","\tAdding images folder: /content/dataset/New_Data_CoV2/Healthy/Patient (45)\n","\tAdding images folder: /content/dataset/New_Data_CoV2/Covid/Patient (31)\n","\tAdding images folder: /content/dataset/New_Data_CoV2/Healthy/Patient (28)\n","\tAdding images folder: /content/dataset/New_Data_CoV2/Healthy/Patient (46)\n","\tAdding images folder: /content/dataset/New_Data_CoV2/Healthy/Patient (7)\n","\tAdding images folder: /content/dataset/New_Data_CoV2/Covid/Patient (37)\n","\tAdding images folder: /content/dataset/New_Data_CoV2/Healthy/Patient (17)\n","\tAdding images folder: /content/dataset/New_Data_CoV2/Covid/Patient (49)\n","\tAdding images folder: /content/dataset/New_Data_CoV2/Covid/Patient (26)\n","\tAdding images folder: /content/dataset/New_Data_CoV2/Covid/Patient (13)\n","\tAdding images folder: /content/dataset/New_Data_CoV2/Healthy/Patient (20)\n","\tAdding images folder: /content/dataset/New_Data_CoV2/Covid/Patient (45)\n","\tAdding images folder: /content/dataset/New_Data_CoV2/Healthy/Patient (21)\n","\tAdding images folder: /content/dataset/New_Data_CoV2/Healthy/Patient\n","\tAdding images folder: /content/dataset/New_Data_CoV2/Covid/Patient (44)\n","\tAdding images folder: /content/dataset/New_Data_CoV2/Healthy/Patient (35)\n"]}]},{"cell_type":"markdown","metadata":{"id":"REK4qvtvPPmj"},"source":["### ***ML MODELS***"]},{"cell_type":"markdown","metadata":{"id":"rXuoRqxb7kq5"},"source":["Single Decision Tree Classifer"]},{"cell_type":"code","metadata":{"id":"BYg7ECjA0Bb4"},"source":["from sklearn.tree import DecisionTreeClassifier\n","from sklearn.metrics import confusion_matrix\n","from sklearn import tree\n","from sklearn.metrics import accuracy_score\n","from sklearn.tree import export_graphviz\n","import graphviz\n","from sklearn.model_selection import cross_val_score\n","from sklearn.model_selection import cross_val_predict"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"apK-Mv7HSSfE","colab":{"base_uri":"https://localhost:8080/","height":128},"executionInfo":{"status":"error","timestamp":1621403172592,"user_tz":240,"elapsed":530,"user":{"displayName":"Matthew Boraske","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gjuu-JGZEUwBYJ4mDYIPdXiP3VutrpOg_K7sJDIvQ=s64","userId":"02919232727119503184"}},"outputId":"299cdcfe-bcd3-4690-e031-2af08a678504"},"source":["#tuning parameters of decision tree by trying different combos of max_depth and max_features\n","############################################################################################\n","\n","depths = [3,5,7,9]\n","feature_amts = [10,50,100,500,1000,2000]\n","\n","best_accuracy = 0\n","best_depth = 0\n","best_feature_amt = 0\n","\n","for depth in depths:\n","  for feature_amt in feature_amts:\n","\n","    #create decision tree\n","    print(\"Decision tree where max depth =\", depth, \"and max features =\", feature_amt)\n","    dt = DecisionTreeClassifier(max_depth=depth, max_features=feature_amt)\n","\n","    #train decision tree\n","    dt.fit(train_image_matrix, train_target)\n","\n","    #create predictions using decision tree\n","    y_pred = dt.predict(test_image_matrix)\n","\n","    #accuracy   \n","    print(\"\\tAccuracy =\", accuracy_score(test_target, y_pred)\n","    if accuracy_score(test_target, y_pred) > best_accuracy:\n","      best_accuracy = accuracy_score(test_target, y_pred)\n","      best_depth = depth\n","      best_feature_amt = feature_amt\n","\n","print(\"\\nBest Accuracy =\", best_accuracy)\n","print(\"\\tMax Depth =\", best_depth)\n","print(\"\\tMax features =\", best_feature_amt)"],"execution_count":null,"outputs":[{"output_type":"error","ename":"SyntaxError","evalue":"ignored","traceback":["\u001b[0;36m  File \u001b[0;32m\"<ipython-input-33-d6bb5e2547e7>\"\u001b[0;36m, line \u001b[0;32m26\u001b[0m\n\u001b[0;31m    if accuracy_score(test_target, y_pred) > best_accuracy:\u001b[0m\n\u001b[0m                                                          ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"]}]},{"cell_type":"code","metadata":{"id":"xe6iIEJ4eAhs","colab":{"base_uri":"https://localhost:8080/","height":229},"executionInfo":{"status":"error","timestamp":1621398433624,"user_tz":240,"elapsed":530,"user":{"displayName":"Matthew Boraske","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gjuu-JGZEUwBYJ4mDYIPdXiP3VutrpOg_K7sJDIvQ=s64","userId":"02919232727119503184"}},"outputId":"b802abbc-b115-4fa0-8135-e7d8e53caa86"},"source":["#create decision tree using best found configuration of parameters (aka the optimal decision tree)\n","##################################################################################################\n","\n","#create decision tree\n","dt = DecisionTreeClassifier(max_depth=5, max_features=2000)\n","\n","#train decision tree\n","dt.fit(x_train, y_train)\n","\n","#create predictions using decision tree\n","y_pred = dt.predict(x_test)\n","\n","### Performance metrics ###\n","#confusion matrix\n","print(confusion_matrix(y_test, y_pred))\n","\n","#accuracy\n","acc = accuracy_score(y_test, y_pred)\n","print(\"Accuracy of optimal decision tree =\", acc)"],"execution_count":null,"outputs":[{"output_type":"error","ename":"NameError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-17-c65621519041>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;31m#train decision tree\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0mdt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;31m#create predictions using decision tree\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mNameError\u001b[0m: name 'x_train' is not defined"]}]},{"cell_type":"code","metadata":{"id":"QPevh_wWORmP"},"source":["#save optimal decision tree to a .png file\n","##########################################\n","\n","export_graphviz(dt, out_file=\"mytree.dot\")\n","with open(\"mytree.dot\") as f:\n","    dot_graph = f.read()\n","graphviz.Source(dot_graph)\n","\n","#convert .dot to .png and save it\n","!dot mytree.dot -Tpng -o NewDecisionTree.png"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"p_ju9oW0EdkO"},"source":["Random Forest Classifier"]},{"cell_type":"code","metadata":{"id":"wO6eLTBLM1Zh"},"source":["from sklearn.ensemble import RandomForestClassifier"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"lEwAz0GJEhoB"},"source":["#determine what max depth for RF trees achieves best accuracy\n","#############################################################\n","\n","best_acc = 0\n","best_depth = 0\n","depths = [1,2,3,4,5,6,7,8,9]\n","accuracies = []\n","\n","for depth in depths:\n","\n","  #create and train RF \n","  print(\"\\nTraining random forest with max depth of\",depth)\n","  rf = RandomForestClassifier(max_depth=depth, random_state=0)\n","\n","  #train decision tree\n","  scores = cross_val_score(rf, image_matrices, image_targets, cv=5)\n","\n","  #accuracy   \n","  print(\"\\tAccuracy =\", scores.mean())\n","  accuracies.append(scores.mean())\n","  \n","  if scores.mean() > best_acc:\n","    best_acc = scores.mean()\n","    best_depth = depth \n","\n","print(\"\\nBest accuracy achieved is\", best_acc, \"using a max depth of\", best_depth)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"98OPJVqrG3uJ"},"source":["#plot accuracies over max depth\n","###############################\n","\n","x_vals = range(1,10)\n","plt.plot(x_vals,accuracies)\n","plt.title(\"Accuracies of Random Forests with Varying Max Depths\")\n","plt.xlabel(\"Max Depth\")\n","plt.ylabel(\"Accuracy\")"],"execution_count":null,"outputs":[]}]}